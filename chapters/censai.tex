
%\begin{document}

\chapter{Pixelated Reconstruction of Foreground Density and Background Surface Brightness in Gravitational Lensing Systems using Recurrent Inference Machines}\label{chap:censai}
\thispagestyle{empty}

\begin{center}
Alexandre Adam,$^{1,2}$
Laurence Perreault-Levasseur,$^{1,2,3}$
Yashar Hevazeh$^{1,3}$
\end{center}

\vspace*{0.5cm}
\noindent $^{1}$\textit{D\'{e}partement de physique, Universit\'{e} de Montr\'{e}al, Montr\'{e}al, H3C 3J7, Canada}\\
$^{2}$\textit{Mila - Quebec Artificial Intelligence Institute, Montréal, Canada}\\
$^{3}$\textit{Center for Computational Astrophysics, Flatiron Institute, 162 5th Avenue, 10010, New York, NY, USA}\\
\vspace*{1.5cm}

\begin{center}
Un résumé de cet article à été accepté à l'atelier \textit{Machine Learning for Astrophysics
Workshop at the Thirty-ninth International Conference on Machine Learning (ICML 2022)}. \\
Cet article sera soumis à la revue \textit{The Astrophysical Journal} (ApJ) durant le prochains mois. 
\end{center}


\clearpage
%\begin{resume}
\section*{Résumé}
Modéliser les lentilles gravitationnelles dans le but de quantifier les distorsions des images 
d'arrière-plan et de reconstruire la densité de masse de la lentille en avant-plan est encore aujourd'hui 
un problème difficile, posant un défi computationnel majeur. Avec le nombre croissant de lentilles découvertes et 
la résolution croissante des images de ces systèmes, la tâche d'exploiter complètement l'information qu'elles contiennent 
est présentement un problème hors d'atteinte pour les algorithmes traditionnels.
Dans ce travail, on introduit un réseau neuronal récurrent basé sur les machines à inférence récurentielles (RIM) 
pour reconstruire simultanément une image non déformée de la source en arrière-plan et une image de la densité de masse de la lentille. 
La méthode que nous présentons reconstruit de façon itérative les paramètres du modèle (les pixels de la source et de la densité de la lentille) 
en apprenant le processus d'optimisation de la vraisemblance étant donné une observation et un modèle physique (une simulation des chemins lumineux), 
régularisée par des biais inductifs appris implicitement par le réseau de neurones avec les données d'entraînement. 
Comparée aux méthodes traditionnelles basées sur des modèles paramétriques de la densité de masse, notre approche 
est significativement plus expressive et peut reconstruire des distributions de masses complexes, ce qu'on démontre 
en utilisant des galaxies lentilles réalistes provenant de la simulation cosmologique hydrodynamique IllustrisTNG.

\textbf{Mots-clés:} Lentilles gravitationnelles ---
        Simulations astrophysiques  ---
        Inférence non-paramétrique ---
        Réseaux neuronaux convolutifs.
%\end{resume}


\section*{Abstract}
Modeling strong gravitational lenses in order to 
quantify the distortions in the images of background sources and 
to reconstruct the mass density in the foreground lenses has been a difficult computational challenge. 
As the quality of gravitational lens images increases, the task of fully exploiting the information they contain 
becomes computationally and algorithmically more difficult. 
In this work, we use a neural network based on the Recurrent Inference Machine (RIM) to simultaneously reconstruct an undistorted 
image of the background source and the lens mass density distribution as pixelated maps. 
The method iteratively reconstructs the model parameters (the image of the source and a pixelated density map) by learning 
the process of optimizing the likelihood given the data using the physical model (a ray-tracing simulation), regularized
by a prior implicitly learned by the neural network through its training data. When compared to more traditional parametric models, 
the proposed method is significantly more expressive and can reconstruct complex mass distributions, 
which we demonstrate by using realistic lensing galaxies taken from the IllustrisTNG cosmological hydrodynamic simulation. 

\textbf{Keywords:} Gravitational lensing (670) ---
        Astronomical simulations (1857) ---
        Nonparametric inference (1903) ---
        Convolutional Neural Networks (1938).


\section{Introduction}

Strong gravitational lensing is a natural phenomenon through which multiple, distorted images of luminous background sources are formed by the gravity of  massive foreground objects along the line of sight 
\citep[e.g.,][]{Viera2013,Marrone2018,Rizzo2020,Sun2021}. 
These distortions are tracers of the distribution of mass in foreground structures, irrespective of their light emission properties. 
As such, this phenomenon offers a powerful probe of the distribution of 
dark matter \citep[e.g.,][]{Dala2002,Treu2004,Hezaveh2016,Gilman2020,Gilman2021}.

Lens modeling is the process through which the parameters describing both the mass distribution in the 
foreground lens and the undistorted image of the background source are inferred.
This has traditionally been done through explicit likelihood-based modeling methods, a time- and resource-consuming procedure. 
A common practice to model strong lenses is to model the light profile of the background source with a \citet{Sersic1963} profile and the density of the foreground lens with a power law function, $\rho \propto r^{-\gamma}$. 
These simple profiles allow for the exploration of their low-dimensional parameter space with non-linear samplers such as Marchov Chain Monte Carlo (MCMC) methods \citep[e.g.,][]{Koopmans2006,Barnabe2009,Auger2010} and generally provide a good fit to low-resolution data. 
However, as high-resolution and high signal-to-noise ratio (SNR) images become available, lensing analysis with simple models requires the introduction of additional parameters representing the true complexity of the mass distribution in lensing galaxies and the complexity of surface brightness in the background sources \citep[e.g.,][]{Sluse2017,Wong2017,Birrer2019,Rusu2019, Rusu2017,Li2021}. 
This approach becomes intractable as the complexity of the mass distribution and the quality of images increases \citep[e.g.,][]{Schmidt2022}. For example,
no simple parametric model of the \textit{Hubble Space Telescope} (\textit{HST}) images of the Cosmic Horseshoe (J1148+1930) --- initially discovered by \citet{Belokurov2007} --- has been able to model the fine features of the extended arc \citep[e.g.,][]{Bellagamba2016,Cheng2019,Schuldt2019}.

Free-form methods attempt to relax the assumptions about the smoothness and symmetries of these parametric profiles
using more expressive families like regular (or adaptive)
grid representations and meshfree representations
\citep{Saha1997,Abdelsalam1998,Abdelsalam1998b,Diego2005,Birrer2015,Merten2016}. 
These methods strive to model the signal contained in lensed images in a data-agnostic way, in order to place better constraints on the morphology of the source brightness or the 
projected mass density of the lens. 
However, most free-form parametrization choices make the inference problem under-constrained, meaning that imposing a prior on the reconstructed parameters becomes essential to penalize unphysical solutions and avoid overfitting the data.
% \citep{Bartelmann1996,Saha1997,Seitz1998,Abdelsalam1998,Abdelsalam1998b,Diego2005,Diego2007,Liesenborgs2006,Liesenborgs2007,Coe2008,Merten2009,Birrer2015,Merten2016,Torres-Ballestros2022}. 
% Kaiser And Squires Mass mapping weak lensing (leading up to Remy2022 - score based method reinforcing results from deep learning obtain 2 year prior by same group, and other method cited in this paper)
% Bartelmann 1996 (cluster lensing free-form reconstruction, with a priori known 10^5 galaxy source as background -> reconstruct potential starting from smoothed prior)
% Abdelsalam et al. (1998)
% Saha & Williams (1997)
% Seitz & Schneider (1998) Maximum entropy regularisation (cluster lensing, free-form inversion of the kappa map)
% Cacciato, Bartelmann 2006 -> combine weak and strong lensing info in cluster to reconstruct 2d potential, focus on using arcs to constrain the location of critical curves (very similar to Bradac work, except this critical curve thing)
% Jee et al 2007: Discovery of a ring-like dark matter structure in the cluster  Cl 0024+17 (pretty cool)
% Deb, Goldberg 2008: Particle Based Lensing, used for cluster reconstruction, can include all information like weak lensing, strong lensing constraints (image positions, flux, known position of critical curves)
% Liesenbourg (2006, -07, -09, -20): GRALE, genetic algorithm to infer kappa for cluster lensing by summing over basis function (Plummer profiles)
% Gosh et al 2020: GRALE can reconstruct cluster with up to 1000 multiple images for ultra deep fields images of certain clusters (e.g. w/ JWST)
% Diego et al 2005, 2007: WSLAP - probably the only method using pixel with no known arcs to constrain further the free-form mass model (mutliresolution grid parameter family)
% Bradac 2005a,b and 2009: SWUnited: combine weak and strong lensing info to break the mass sheet degeneracy for cluster lensing
% Coe et al 2008: LensPerfect - perfect reproduction of image position, flux and shear for cluster lenses. Explore thoroughly analytical priors for mass maps etc.
% Mertens 2009, -11, -16 SaWLens, 2009 explore the introduction of regularisation into the Weak + Strong lensing constraints, 2011: application to real data, 2016: mesh-free reconstructions.

In the context of traditional likelihood-based modeling, there exists a number of commonly used priors for the inference of high dimensional representations of background sources (e.g., imposing a quadratic-log prior for linear inversion of pixellated-source models as developed by \citet{Warren2003,Suyu2006} or iteratively specified priors for shapelets \citep{Birrer2015,Birrer2018,Nightingale2018}).
However, these priors are often simplistic approximations to the actual distribution of pixel brightness in unlensed galaxies, and thus can result in important, difficult to characterize biases.

On the other hand, for lens mass reconstruction, the issue of specifying an appropriate prior is still unsolved. This has been studied extensively in the context of cluster-scale strong lensing \citep{Bartelmann1996,Seitz1998,Abdelsalam1998,Abdelsalam1998b,Bradac2005,Diego2005,Cacciato2006,Diego2007,Liesenborgs2006,Liesenborgs2007,Jee2007,Coe2008,Merten2009,Deb2012,Merten2016,Ghosh2020,Torres-Ballestros2022}. Free-form approaches in the context of strong galaxy-galaxy lenses have been comparatively less studied (see however \citet{Saha1997,Saha2004,Birrer2015,Coles2014}). 


Another major challenge for these models is the issue of optimizing or sampling these high dimensional posteriors. 
Given the non-linear nature of the model and the existence of multiple local optima, non-linear global optimizers and samplers are needed, which often results in extremely expensive computational procedures.
The high computational cost of these methods also limits the extent to which they can be thoroughly tested and validated to identify and characterize potential systematics.


Over the recent years, deep learning methods have proven extremely successful at accurate modeling of strong lensing systems \citep{Hezaveh2017,PerreaultLevasseur2017,Morningstar2018,Coogan2020,Park2021,Legin2021,Legin2022,Wagner-Carena2021,Schuldt2022,Wagner-Carena2022,Karchev2022,AnauMontel2022,Mishra-Sharma2022,Schuldt2022}.
More specifically, \citet{Morningstar2019} demonstrated that recurrent convolutional neural networks can implicitly learn complex prior distributions from their training data to successfully reconstruct pixelated undistorted images of strongly lensed sources, circumventing the need to explicitly specify a prior distribution over those parameters. Motivated by this, we propose a method that extends this framework to solve the full lensing problem and simultaneously reconstruct a pixelated mass map and a pixelated image of the undistorted background source.

%In this work, we propose a method for pixelated 
%strong gravitational lensing mass and source reconstruction, 
%allowing it to reconstruct complex distributions. 
The method we propose here is based on the Recurrent Inference Machine (RIM), originally developed by \citet{Putzky2017}. In its original version, this method proposed to solve inverse problems using a Recurrent Neural Network as a metalearner to learn the iterative process of the optimization of a likelihood. RIMs have been trained on a range of linear inverse problems both within and outside of astrophysics \citep{Lonning2019}. In \cite{Modi2021}, this method was generalized to non-linear inference problems while using a U-net architecture \citep{Ronneberger2015} to separate the dynamics of different scales. 

In the present paper, we leverage this framework to learn an optimization process over the highly non-convex strong lensing likelihood, and implicitly learn a data-driven prior, which allows for the reconstruction of complex mass distributions representative of realistic galaxies taken from the IllustrisTNG \citep{Nelson2018} hydrodynamical simulations.
% In this framework, we aim to learn an iterative inference algorithm, moving away 
% from hand-chosen inference algorithms and hand-crafted priors. 
% Instead, the prior is learned implicitly through the dataset used to train 
% the neural network that update the solution parameters at each iteration. 
% In this paper, we present a new architecture for the neural network based on a U-net architecture \citep{Ronneberger2015}, tailored to our highly non-linear inverse problem. 
We also introduce a fine-tuning procedure, which allows us to directly exploit the prior encoded in the neural network parameters in order to further optimize the posterior down to noise levels. We apply this to the reconstruction of high signal-to-noise galaxy-galaxy lensing systems simulated using IllustrisTNG \citep{Nelson2018} projected density maps and background galaxy images collected from the COSMOS survey \citep{Koekemoer2007,Scoville2007}.

The paper is organised as follows. Section \ref{sec:methods} details 
the inference pipeline. In Section \ref{sec:data}, we present the data production and preprocessing for the training of the RIM and the generative models used in this paper. In Section \ref{sec:training}, 
we report on the training strategies used. 
In Section \ref{sec:results},
we discuss our results on a test set of gravitational lenses. We conclude in Section 
\ref{sec:conclusion}.


\section{Methods}\label{sec:methods}
Our goal is to predict pixelated maps of both the undistorted image of the background source and the projected density in the foreground lens from noisy lensed images. Our model consists of a Recurrent Inference Machine that predicts these variables of interest. Training this model requires large number of training data, which we produce using a Variational Autoencoder (VAE) trained on density maps from the IluustrisTNG simulation and background sources from the Cosmos dataset (Section \ref{sec:vae training}).  

In this section, we present the structure of the lensing inference problem and provide information about our analysis method.
We begin with a general introduction to maximum a posteriori (MAP) inference in Section \ref{sec:maximum a posteriori}.
We describe the lensing simulation pipeline in Section \ref{sec:forward model}.
In Section \ref{sec:rim}, we motivate the use of the Recurrent Inference Machine and describe its  computational graph.
The architecture of the neural network is described in Section 
\ref{sec:gradient model}.  Finally, we describe the fine-tuning procedure and the transfer learning technique applied to achieve noise-level reconstructions in Section \ref{sec:fine-tuning}.


\subsection{Maximum a posteriori inference}\label{sec:maximum a posteriori}


We consider the task of reconstructing a vector of parameters of interest $\mathbf{x} \in \mathcal{X}$ given a vector of noisy
observed data $\mathbf{y} \in \mathcal{Y}$, a known forward (or physical) model $F$, and an additive noise vector $\boldsymbol{\eta}$. 
In what follows, we assume this vector to be sampled from a Gaussian distribution with known covariance matrix $C$, 
such that we can write
\begin{equation}\label{eq:MainEquation} 
\begin{aligned}
        \mathbf{y} &= F(\mathbf{x}) + \boldsymbol{\eta}\, ;\\[2pt]
        \boldsymbol{\eta} &\sim \mathcal{N}(0, C)\, .
\end{aligned}
\end{equation} 
In our case study, $F$ is a many-to-one non-linear 
mapping between the parameter space $\mathcal{X}$ 
and the data space $\mathcal{Y}$. 
Finding physically allowed solutions for this ill-posed inverse problem requires strong priors. The maximum a posteriori (MAP) solution maximizes the product of the likelihood $p(\mathbf{y} \mid \mathbf{x})$ and the prior $p(\mathbf{x})$:
\begin{equation}\label{eq:Posterior} 
        \hat{\mathbf{x}}_{\mathrm{MAP}} = \underset{\mathbf{x} \in \mathcal{X}}{ \mathrm{argmax}}\,\,
        \log p(\mathbf{y} \mid \mathbf{x}) + \log p(\mathbf{x})\, .
\end{equation} 
Assuming a Gaussian noise model for $\boldsymbol{\eta}$, the log-likelihood can be written analytically as
\begin{equation}\label{eq:Likelihood} 
        \log p(\mathbf{y} \mid \mathbf{x}) \propto -
        %\underbrace{
        \big(\mathbf{y} - F(\mathbf{x})\big)^{T} C^{-1} \big(\mathbf{y} - F(\mathbf{x})\big)\, .
%}_{\displaystyle \chi^2}.
\end{equation} 
The prior distribution, however, is problem-dependent and encodes
expert knowledge of the model domain. As such, it is typically harder to write explicitly. 

\subsection{The Forward Model}\label{sec:forward model}

The forward model, $F$, is a simulation pipeline that receives a map of the surface brightness in the background source and a map of the projected density in the foreground lens to produce distorted images of background galaxies.
This pipeline uses ray tracing to calculate the deflection angles, $\bm{\alpha}$, and maps the observed coordinates, $\bm{\theta}$, into the coordinates of the background plane, $\bm{\beta}$, through the lens equation
\begin{equation}\label{eq:LensEquation}
        \bm{\beta} = \bm{\theta} - \bm{\alpha}(\bm{\theta})\, .
\end{equation}
The deflection angles are obtained using the projected surface 
density field $\kappa$ --- also referred to as convergence --- through the integral
\begin{equation}\label{eq:alpha}
        \bm{\alpha}(\boldsymbol{\theta}) = 
        \frac{1}{\pi} \int_{\mathbb{R}^2}
        \kappa(\boldsymbol{\theta}') 
        %\underbrace{
        \frac{\boldsymbol{\theta}
        - \boldsymbol{\theta}'}{\lVert \boldsymbol{\theta} - 
        \boldsymbol{\theta}' \rVert^2}
%}_{\displaystyle \boldsymbol{\omega}(\boldsymbol{\theta_i} -\boldsymbol{\theta'})} 
        d^2\boldsymbol{\theta}'\, .
\end{equation}
The intensity of a pixel in a simulated observation
is obtained through bilinear interpolation of the 
source brightness distribution at the coordinate $\boldsymbol{\beta}$.
Since we also use a discrete representation for the convergence, 
we approximate this integral by a discrete global convolution. Taking 
advantage of the convolution theorem, this operation 
can be computed in near-linear time using 
the Fast Fourier Transform algorithm (FFT). 

Assuming the observation 
has $M^2$ pixels, the convolution kernel %$\boldsymbol{\omega}$ 
would have $(2M + 1)^{2}$ pixels. 
Both the convergence map and the kernel are zero-padded 
to a size of $(4M+1)^{2}$ pixels in order to approximate a linear 
convolution and significantly reduce aliasing.

To produce simulated images, a blurring operator --- convolution by a point spread function (PSF) --- is 
applied to the lensed image to replicate the response of an optical system.
This operator is implemented as a GPU-accelerated matrix operation 
since the blurring kernels used in this paper have a significant proportion
of their energy distribution encircled inside a small pixel radius. Gaussian noise is then applied to the images, as described in more details in section \ref{sec:simulated observation}.
% \vfill\null


\subsection{Recurrent Inference Machine}\label{sec:rim}

Instead of handcrafting a prior distribution to solve the inverse problem (\ref{eq:MainEquation}), we build an inference pipeline with
a data-driven implicit prior encoded in a deep neural network architecture \citep{Bengio2009}.
The RIM \citep{Putzky2017} is a form of learnt
gradient-based inference algorithm, intended to solve inverse problems of the form  \eqref{eq:MainEquation}. This framework has mainly been applied in the context of linear under-constrained inverse problems --- i.e.\ where $F(\mathbf{x})$ can be represented as a matrix product $A\mathbf{x}$ --- for which the prior on the parameters $\mathbf{x}$, $p(\mathbf{x})$, is 
either intractable or hard to compute \citep{Morningstar2018,Morningstar2019,Lonning2019}. 
The use of the RIM to solve non-linear inverse problems was first investigated in \citep{Modi2021}.
In our case, the function representing the physical model $F$ encodes the lens equation (\ref{eq:LensEquation}), which is highly non-linear. 



 The RIM is made up of a recurrent unit, which, given an observation $\mathbf{y}$, solves (\ref{eq:MainEquation}) for $\mathbf{x}$ through the governing equation
\begin{equation}\label{eq:RIM}
        \mathbf{\hat{x}}^{(t+1)} = \mathbf{\hat{x}}^{(t)} 
        + g_\varphi \big(\mathbf{\hat{x}}^{(t)},\, \mathbf{y},\, 
\grad_{\mathbf{\hat{x}^{(t)}}} \log p(\mathbf{y} \mid \mathbf{\hat{x}}^{(t)})\big)\, ,
\end{equation}
where $\mathbf{\hat{x}}^{(t)}$ is the estimate of the parameters of interest at time $t$ of the recursion (here, the pixel values of the image of the undistorted background source and of the density field $\kappa$) and $g_\varphi$ is a neural network. In the text, we will often use the shorthand notation $\grad_{\mathbf{y} \mid \mathbf{x}}$ to refer to $\grad_{\mathbf{x}} \log p(\mathbf{y} \mid \mathbf{x})$, the gradient of the likelihood evaluated at $\mathbf{x}$.
By minimizing a weighted mean squared loss backpropagated 
through time, 
\begin{equation}\label{eq:Loss}
		\mathcal{L}_\varphi(\mathbf{x}, \mathbf{y}) = 
		\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{M} \mathbf{w}_i (\mathbf{\hat{x}}^{(t)}_i - \mathbf{x}_i)^2\, ,
\end{equation} 
where $T$ is the total number of time steps in the recursion, the index $i$ labels the pixels of the reconstructions, $\mathbf{w}_i$ is the per-pixel weight, and $M$ is the total number of pixels in the reconstructions, the neural network $g_\varphi$ learns to optimize the parameters $\mathbf{x}$ given a likelihood function. 
The converged parameters of the neural network given the training set $\mathcal{D}$, 
$\varphi^{\star}_{\mathcal{D}}$, are those that minimize the cost --- or empirical risk ---
which is defined as the 
expectation of the loss over $\mathcal{D}$
\begin{equation}\label{eq:Cost} 
		\varphi^{\star}_{\mathcal{D}} = \underset{\varphi}{\text{argmin}}\,\,
		\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim \mathcal{D}}\big[
		\mathcal{L}_\varphi(\mathbf{x}, \mathbf{y}) \big].
\end{equation} 
Unlike previous works 
\citep{Andrychowicz2016,Putzky2017,Morningstar2018,Morningstar2019,Lonning2019}, 
the data vector $\mathbf{y}$ containing the observations 
is fed to the neural network in order to learn 
a better initialization of the parameters, $\mathbf{x}^{(0)} = g_\varphi(0, \mathbf{y}, 0)$, 
in addition to their optimization process. 
 Empirically, we found that this significantly improves the performance 
of the model for our problem and avoids situations where 
the model would get stuck in local minima at 
test time due to poor initialization. 
\begin{figure}[t!]
        \centering
		\includegraphics[width=0.7\linewidth]{figures/schematic_rim}
        \caption{Rolled computational graph of the RIM. Dashed arrows represent operations not recorded for BPTT.}
        \label{fig:rolled graph}
\end{figure}


We follow previous works in setting a uniform weight over the time 
steps ($\mathbf{w}^{(t)} = \frac{\mathbf{w}}{T}$). 
The choice of the pixel weights $\mathbf{w}_i$ is informed
by our empirical observations when training the network. Details are reported in appendix \ref{ap:rim training and opt}.

%In figure \ref{fig:unrolled_graph}, we show the unrolled computational graph of the RIM. 
In Figure \ref{fig:rolled graph}, we show the rolled computational graph of the 
RIM. During training of the neural network $g_\varphi$, operations along the solid arrows are being 
recorded for backpropagation through time. 
The recording is stopped along the dashed arrow since these operations 
are part of the forward modelling process and contain no trainable parameters.
%By avoiding the computation of these gradients, training time is reduced and 
%knowledge about the inner workings  
%of a specific likelihood (and forward model) is insulated from the optimization algorithm.
%This is analogous to a common RNN use-case like text generation, where the process responsible 
%for producing the next element in a time series is a black box to the optimization 
%algorithm. 

The gradient of the likelihood is computed using automatic differentiation. Following 
\citep{Modi2021}, we preprocess the gradients using the Adam algorithm \citep{Kingma2014}. 
For clarity, we only illustrate this step in Figure \ref{fig:unet}. 
% \vfill\null

\begin{figure*}[ht!]
        \centering
        \includegraphics[width=\textwidth]{figures/unet_architecture.pdf}
        \caption{
A single time step of the unrolled computation graph of the RIM.
GRU units are placed in the skip connections to guide the 
reconstruction of the source and convergence. A schematic of the steps to compute 
the likelihood gradients is shown in the bottom right of the figure, including the 
Adam processing step of the likelihood gradient.}
        \label{fig:unet}
\end{figure*}

\subsection{The Neural Network}\label{sec:gradient model}


The neural network architecture is illustrated in Figure \ref{fig:unet}, which shows 
a single time step of the unrolled computation graph of the RIM.
We use a U-net \citep{Ronneberger2015} architecture 
with Gated Recurrent Units \citep[GRU:][]{Cho2014} placed in each skip connection. 

Each GRU cell has its own memory tensor that is updated through time at each iteration of 
equation \ref{eq:RIM}. The shape of a memory tensor is set to match the
feature tensor fed into it from the parent layer in the network graph. 
Instead of learning a compressed representation like in the hourglass
architecture (or autoencoder), the U-net architecture naturally separates the spatial 
frequency components of the signal into its vertical levels. The first level generally encodes 
high frequency features while the lower levels encode low frequency features (due to downsampling of the feature maps). 
Adding an independent memory unit 
at each level preserve this property.

Convolutional layers with a stride of 2 are used for downsampling and 
stride of $\frac{1}{2}$ for upsampling of the feature maps
(identified in blue and orange respectively in figure \ref{fig:unet}). Half-stride convolutions are implemented in practice with the transposed convolution layers from \texttt{Tensorflow} \citep{tensorflow}.
Most layers use a kernel size of $3\times3$, except the first and last layer. 
The first layer has 
larger receptive field ($11\times11$) in order to capture more details in the input tensor. 
The last layer has kernels of size $1\times 1$. 
A $\tanh$ 
activation function is used 
for each convolutional layer, including strided convolutions, except for the output 
layer. The U-net outputs an image tensor with two channels, one dedicated for the update of the source 
and the other for the update of the convergence (see figure \ref{fig:unet}). 
% \vfill\null
%\columnbreak


% Talk about this choice
% Shared memory is important, possibly more than the notion that source and kappa require very different 
% reconstruction procedure (because of different structure etc).

% Choice of model correspond to choosing an inductive bias, or how the function should behave 
% for points in and out of the dataset
% Generalization means the ability to make prediction about the behavior of the function 
% at novel points in the domain of the function.
% In the meta-learning framework, examples are problem instances. Generalization means to transfer 
% knowledge accross problem instances.
% The reuse of the problem structure is known as transfer learning. In the context of meta-learning, 
% this is cast as generalization.


\subsection{Fine-Tuning}\label{sec:fine-tuning}

\subsubsection{Objective function}
Once trained, the RIM produces a baseline (point)
estimate of the parameters $\mathbf{x}$ given a noisy observation $\mathbf{y}$, a PSF and a noise covariance matrix. 
%We note $(\mathbf{y},\Pi,C) \sim \mathcal{T}$.
We now concern ourselves with a strategy to improve 
this estimate. 
This is important 
for observations with high SNR, for which the estimate
must be very accurate to model all the fine features present in the arcs.
The metric for the goodness of fit 
is the reduced chi squared $\chi^2_\nu = \frac{\chi^2}{\nu}$, 
where $\nu$ is the total number of degrees of freedom which here corresponds to 
the total number of pixels in $\mathbf{y}$.
Generally, our goal will be to reach $\chi^2_\nu = 1$, or equivalently $|\chi^2 - \nu| = 0$, 
which suggests that the RIM's estimate has modeled all the signal 
to be recovered from the observations. 
We note that such a problem is exceedingly  
difficult at high SNR. 

We note that we can optimize the log-likelihood directly w.r.t.\ the network weights given an appropriate prior on those weights (to avoid forgetting the implicit priors that have been learned during training, see section \ref{sec:transferlearning}). The new objective function is given by
\begin{equation}\label{eq:MAP} 
        \hat{\varphi}_{\mathrm{MAP}} = \underset{\varphi}{\mathrm{argmax}}\,\, 
        \frac{1}{T}\sum_{t=1}^{T} \log p(\mathbf{y} \mid \mathbf{\hat{x}}^{(t)}) + \log p(\varphi) \, ,
\end{equation} 
where $\varphi$ are the network weights, $\log p(\mathbf{y} \mid \mathbf{\hat{x}}^{(t)})$ is the log-likelihood, and $\log p(\varphi) $ is the log prior over the network weights.
Unlike the loss in equation \eqref{eq:Loss}, this objective function makes no use of labels 
($\mathbf{x}$). 
This allows us to use equation \eqref{eq:MAP} at test time in order to fine-tune the RIM's weights to a specific test example. 

\subsubsection{Transfer Learning}
\label{sec:transferlearning}
We now address the issue of transferring knowledge from the training task defined by the loss function in equation \eqref{eq:Cost}, 
to a test task specific to an observation,  as defined by the loss given in equation \eqref{eq:MAP}.
The reader might refer to reviews on transfer learning \citep{Pan2010,Zhuang2019} 
for a broad overview of the field. The strategy we outline falls within 
the category of inductive transfer learning.

%Despite the second term in Eq \eqref{eq:MAP}, we find 

% Since the data likelihood $p(\mathbf{y} \mid \mathbf{x})$ 
% does not contain \textit{a priori} information 
% about the solution $\hat{\varphi}_{\mathrm{MAP}}$,
% inductive biases must be introduced to make 
% the problem \eqref{eq:MAP} well-posed. Thus, we
% \begin{enumerate}[label=(\subscript{\mathcal{H}}{{\arabic*}})]
%         %\setcounter{enumi}{3}
%         \item \label{prior:initialization} initialize the network parameters with $\varphi_{\mathcal{D}}^{\star}$; 
%         \item \label{prior:early stopping} apply early stopping when a maximum number of steps is reached or 
%                 $\chi^2_\nu \leq 1$;
%         % \item \label{prior:learning rate} use a small learning rate.
% \end{enumerate}
% \ref{prior:early stopping} and \ref{prior:learning rate} encode the assumption 
% that the optimal estimator is to be found \textit{near} the initialization.

\begin{figure}[tb!]
       \centering 
       \includegraphics[width=0.8\linewidth]{figures/main_figurev2}
       \caption{Example of a simulated lensed image in the test set that 
exhibits a large deflection in its eastern arc which indicates the presence of a massive object
 --- in this case a dark matter subhalo. The fine-tuning procedure is able to recover 
this subhalo because of its strong signal in the lensed image and reduces the residuals 
to noise level.}
       \label{fig:main figure}
\end{figure}

Optimizing the log-likelihood alone without a prior term over the weights (i.e.~just the first term from the r.h.s.~in \eqref{eq:MAP}) by initializing the weights at $\varphi^\star_\mathcal{D}$ is not strong 
enough to preserve the knowledge learned from the training task. 
This has long been observed in the literature and was coined as the 
catastrophic interference phenomenon in 
connectionist networks \citep{McCloskey1989,Ratcliff1990}.
In summary, a sequential learning problem exhibits catastrophic 
forgetting of old knowledge when confronted with new examples (possibly 
from a different distribution or process), in a manner 
\begin{enumerate}%[label=(\subscript{\mathrm{CF}}{{\arabic*}})]
        \item \label{cf:steps} proportional to the amount of learning;
        \item \label{cf:weights} strongly dependant to the disruption of the parameters
                involved in representing the old knowledge.
\end{enumerate}
While introducing an early stopping condition could 
potentially alleviate the former issue, the latter could still remain a problem.

We therefore follow the work of \citet{Kirkpatrick2016} to define a prior distribution
over $\varphi$ that address this issue
\begin{equation}\label{eq:Prior} 
        \log p(\varphi) \propto -\frac{\lambda}{2}\sum_{j} \mathrm{diag}(\mathcal{I}(\varphi_{\mathcal{D}}^{\star}))_{j} 
        (\varphi_j - [\varphi^{\star}_{\mathcal{D}}]_{j})^{2}\, .
\end{equation} 
where $\mathrm{diag}(\mathcal{I}(\varphi_{\mathcal{D}}^{\star}))$ is the diagonal of the 
Fisher information matrix 
encoding the amount of information that  
some set of gravitational lensing systems from 
the training set, and similar to the observed 
test task, carries about the baseline RIM weights $\varphi_{\mathcal{D}}^{\star}$ 
--- the parameters that minimize the empirical risk (equation \ref{eq:Cost}).
We can also understand this prior using the
Cramér-Rao lower bound 
\citep{Rao1945,Cramer1946}.
%\begin{equation}\label{eq:iCramerRao}
        %\mathrm{Var(\varphi)} \geq \underbrace{(1 + \mathrm{Bias}_{\mathcal{D}}(\varphi))^{2}}_{\lambda_b}\mathcal{I}(\varphi)^{-1}.
%\end{equation} 
The prior can thus be framed as a multivariate 
Gaussian distribution characterised by a diagonal covariance matrix with $\mathrm{diag}(\mathcal{I})$ as its inverse 
and by $\varphi^{\star}_{\mathcal{D}}$ as its first moment. 
Within this view, the  
Lagrange multiplier is 
tuning our estimated uncertainty about the neural network weights 
for the particular task at hand.  
We have included a derivation 
of this term in the appendix \ref{ap:ewc}.

%We define the distribution of examples from $\mathcal{D}$ similar to the test task $\mathcal{T}$ with 
%the surrogate conditional $\tilde{p}(\mathcal{D} \mid \mathcal{T})$. 
Examples are drawn from the set of training examples similar to the test task by sampling the latent space of two variational autoencoders (VAE) that model a distribution over the background sources and the convergence maps respectively (as described in Section \ref{sec:source} and \ref{sec:kappa}) near the baseline prediction of the RIM. In practice, we choose an isotropic Gaussian distribution centered around $\hat{\mathbf{z}}^{(T)}$ --- 
the latent code of the baseline prediction --- as a sampling distribution. While we leave the possibility of improving this choice to future work, it is sufficient for our goals. 
Figure \ref{fig:vae fine-tuning} illustrates examples of what is meant here by \textit{similar}. 

\begin{figure}[t!]
        \centering
        \includegraphics[width=0.8\linewidth]{figures/vae_samples_similar_to_highlight}
        \caption{Examples similar to the test task, also shown in Figure \ref{fig:main figure}. The first column shows the ground truth used to simulate the lensed image. The second column shows the baseline prediction that is then encoded in the latent space of the VAE in order to sample the next 4 columns.
}
        \label{fig:vae fine-tuning}
\end{figure}

%By definition, the Fisher matrix is the 
%expected value of the observed information that 
%the examples carry about $\varphi$. 

%We can, however, make use of theoretical and 
%experimental evidence that overparametrized 
%neural networks exhibits a spectral bias \citep{Rahman2018} 
%toward learning low frequencies first during training. We observe 
%that the baseline model generally 
%provides a coherent prediction ($\gamma(k) = 1$) in the low frequency 
%regime, which is consistent with a spectral bias hypothesis. 
%This suggest a possible approach to ground our method. 
%The MLE-optimal estimator should at least 
%preserve the low frequency features predicted by the baseline. 
%Otherwise, we might suspect that the optimisation procedure 
%has found a degenerate solution that is not consistent with the 
%prior learned by the baseline.

%This also points to interesting strategies that could alleviate 
%overfitting. For instance, freezing the deeper layers of the U-net during 
%fine-tuning might preserve the low frequency features learned during pretraining. 
%This is also suggested as a strong regularization method in \citet{Li2018}. 
%We explore such ideas in the appendice


\section{Data}\label{sec:data}

\subsection{COSMOS}\label{sec:source}
The maps of surface brightness of background sources are taken from the \textit{Hubble Space Telescope} (\textit{HST}) 
Advanced Camera for Surveys Wide Field Channel COSMOS field \citep{Koekemoer2007,Scoville2007},
a $1.64\,\mathrm{deg}^{2}$ contiguous survey acquired in the F814W filter. 
A dataset of magnitude limited ($\mathrm{F814W} < 23.5$) deblended galaxy postage stamps \citep{Leauthaud2007} was compiled as 
part of the GREAT3 challenge \citep{Mandelbaum2014}. The data is 
publicly available \citep{Mandelbaum2012}, and the preprocessing is done through the open-source software 
\texttt{GALSIM} \citep{Rowe2015}. \par

We apply the 
\texttt{marginal} selection criteria (see the \texttt{COSMOSCatalog} class) and impose a flux per image
greater than $50\,\,\mathrm{photons}\,\,\mathrm{cm}^{-2}\,\mathrm{s}^{-1}$. 
This final set has a total of 13\,321 individual images.
Each image is saved as a postage stamp of $158^2$ pixels. 
We then subtract the background from each image, apply a random shift, rotate them by an angle multiple of $90^\circ$, crop them down to $128^{2}$ pixels, and finally normalize them to pixel intensities in the range $[0,1]$. We then train an autoencoder to denoise the galaxy images \citep{Vincent2008,Vincent2010}. More specifically, we use the informational bottleneck principle \citep{Tishby2000} to learn a lossy lower-dimensional representation of the data. For a generic CNN autoencoder, this amount to learning a low-pass frequency filter on the COSMOS dataset. Indeed, CNNs are known to exhibit a spectral bias in their learning phase \citep{Rahaman2018}, which we exploit to our advantage in order to filter pixel noise from the galaxy surface brightness. Furthermore, using an expressive CNN autoencoder produces much less artifacts than a naive implementation of such a low-pass filter --- e.g.\ by masking Fourier modes. 

We split the galaxies into a training set (90\%) and a test set (10\%). 
The augmented training set (${\sim50\,000}$ images) is then used to train a VAE, as described in Section \ref{sec:vae training}, and produce simulated observations to train the RIM.

\begin{figure}[t!]
        \centering
        \includegraphics[width=0.8\linewidth]{figures/gal_vae_sample}
        \caption{Examples of COSMOS galaxy images 
                (top row) and VAE generated samples (bottom row) used as labels in $\mathcal{D}$.}
        \label{fig:source}
\end{figure}


\subsection{IllustrisTNG}\label{sec:kappa}
\subsubsection{Smooth Particle Lensing}\label{sec:SPL}
To compute convergence maps from an N-body simulation, 
we use Kernel Density Estimation to produce smooth densities on a regular grid from discrete simulation particles.
This reduces the particle noise affecting all 
important lensing quantities. At the same time, the choice of the kernel size 
is important to preserve substructures in the 
lens that we might potentially be interested in. Following \citet{Aubert2007,Rau2013}, we use Gaussian smoothing with an adaptive kernel size determined by the distance of the 64\textsuperscript{th} nearest neighbours of 
a given particle $D_{64,i}$. 
\begin{equation}\label{eq:Ksmooth}
\begin{aligned}
    \kappa(\mathbf{x}) &= \frac{1}{\Sigma_{\mathrm{crit}}} \sum_{i=1}^{N_{\mathrm{part}}}
        \frac{m_i}{2 \pi \hat{\ell}^2_i} 
        \exp \left(-\frac{1}{2} \frac{(\mathbf{x} - \mathbf{x}_i)^2}{\hat{\ell}_i^2}  \right) \\
    \hat{\ell}_i &= \sqrt{\frac{103}{1024}}D_{64,i}.
\end{aligned}
\end{equation}
The nearest neighbours are found by fitting a k-d tree ---  implemented in 
\texttt{scikit-learn} \citep{scikit-learn} --- 
to the $N_{\mathrm{part}}$  particles 
in a cylinder centered on the centre of mass of the halo of interest.
The critical surface density is defined as
\begin{equation}\label{eq:Scrit}
\Sigma_{\mathrm{crit}} = \frac{4 \pi G}{c^ 2} \frac{D_\ell D_{\ell s}}{D_s},
\end{equation}
where $D_\ell$, $D_s$ and $D_{\ell s}$ are angular diameter distances to the lens, source and between the lens and the source respectively, 
$G$ is the gravitational constant, and $c$ the speed of light.

\subsubsection{Preprocessing}
The projected surface density maps (convergence) of lensing galaxies 
were made using the redshift $z=0$ snapshot  
of the IllustrisTNG-100 simulation \citep{Nelson2018} 
in order to produce physically realistic realizations of density maps containing dark and baryonic matter.
We selected 1604 halos with the criteria that they have a total
dark matter mass of at least $9\times10^{11} M_{\odot}$. We then collected all 
dark matter, gas, stars and black holes particles from the data in the vicinity of the halo. We then create a smooth projected surface density map as prescribed in section \ref{sec:SPL}.

\begin{figure}[t!]
        \centering
        \includegraphics[width=0.8\linewidth]{figures/kap_vae_sample}
        \caption{Examples of smoothed Illustris TNG100 convergence map (top row) 
        and VAE generated samples (bottom row) used as labels in $\mathcal{D}$.}
        \label{fig:kappa}
\end{figure}

We adopt the $\Lambda$CDM cosmology from 
\citet{PlanckCollaboration2018} with $h=0.68$ to compute 
angular diameter distances. We also fix the 
source redshift to $z_s=1.5$ and the deflector redshift to $z_\ell=0.5$. 
We note that changing the redshifts or the cosmology 
only amount in a rescaling of the $\kappa$ map by a global scalar. Thus, this choice does not change the generality of our method.
The smoothed density maps from equation \eqref{eq:Ksmooth} are 
rendered into a regular grid of $188^2$ pixels with a comoving field of view of $105\,\,\mathrm{kpc}/h$. 
To avoid edge effects in the pixelated maps, 
we include particles outside of the field of view in the sum of equation \eqref{eq:Ksmooth}.
\par
Before applying augmentation or considering different projections, our dataset of halos is split into a 
training set (90\%) and a test set (10\%), in order to make sure that the test set consists only 
of convergence maps unseen by the RIM during training.
We take 3 different projections ($xy$, $xz$ and $yz$) of each 3D particle 
distribution, which amounts to a dataset with a total of $4\,812$ individual convergence maps. 
Random rotations by an angle multiple of $90^{\circ}$ and random shifts to the pixel coordinates 
are applied to each image. The $\kappa$ maps are then rescaled by a random factor to change their 
estimated Einstein radius to the range 
$[0.5,\,2.5]$ arcseconds.
The Einstein radius is defined as
\begin{equation}\label{eq:ThetaE}
        \theta_E = \sqrt{\frac{4GM(\theta_E)}{c^ 2} \frac{D_{\ell s}}{D_\ell D_s}}
\end{equation} 
where $M(\theta_E)$ is the mass enclosed inside the Einstein radius. In practice, we estimate this quantity 
by summing over the mass of pixels with a value greater than the critical density ($\kappa > 1$). 
For data augmentation purposes, this procedure gives a good enough estimate of the lensed image separation resulting from a given $\kappa$ map. 
We test multiple scaling factors for each $\kappa$ map, then uniformly sample between those that produce an estimated 
Einstein radius within the 
desired range. This step is used to remove any bias in the Einstein radius that might come from the mass function 
of the simulation.

The final maps are cropped down to $128^2$ pixels.
Placed at a redshift $z_\ell=0.5$, a $\kappa$ map will thus span an angular field of view of $7.69''$ with 
a resolution similar to \textit{HST}. 
%This field of view is wide enough to cover a typical gravitational 
%lens observed in the sky, which partly justify our choice for the comoving field of view earlier. 
With these augmentation procedures, a total of $50\,000$ maps are created from the training split
to train a VAE, as described in Section \ref{sec:vae training}, and produce simulated observations to train the RIM. 

\subsection{Simulated Observations}\label{sec:simulated observation}
Having defined a source map and a convergence map, we apply the ray tracing simulation 
described in section \ref{sec:forward model} to produce a lensed image. 

For each lensed image, a Gaussian PSF is 
created with a full width at half maximum (FWHM) 
randomly generated from a truncated normal distribution.
The support of the distribution is truncated below by the 
angular size of a single pixel and above by the angular size of 4 pixels. 
White noise with a standard deviation randomly generated from a truncated normal distribution 
is then added to the convolved lensed image to simulate noisy observations. These noise realizations result in SNRs between 
$10$ and $1000$. For simplicity, we define $\mathrm{SNR} = \frac{1}{\sigma}$. 
This definition is equivalent to the peak signal-to-noise ratio. 

%We set the observed image field of view to match with the convergence field view ($7.69''$). 
%We choose the background field of view to be $3''$.
To ensure that the images are representative of strongly lensed source, we require a minimum flux magnification of 3. We
also make sure that most pixel coordinates in the image plane are mapped inside the 
source coordinate system through the lens equation \eqref{eq:LensEquation}. 

\begin{table}[htb!]
\centering
\caption{Physical model parameters.}
\label{tab:phys}
\begin{tabular}{ccc}
        Parameter &  Distribution/Value \\
        \hline \hline
        Lens redshift $z_\ell$ & $0.5$ \\
        Source redshift $z_s$ & $1.5$ \\
        Field of view ('') & $7.69$ \\
        Source field of view ('') & $3$ \\
        PSF FWHM ('') & $\mathcal{TN}(0.06,\, 0.3;\, 0.08,\, 0.05)$
        \footnote{We defined the parameters of the truncated normal in the order $\mathcal{TN}(a,\, b;\, \mu,\, \sigma)$, where $[a,\, b]$ defines the support of the distribution.} \\
        Noise amplitude $\sigma$ & $\mathcal{TN}(0.001,\, 0.1;\, 0.01,\,0.03)$\\
        \hline
\end{tabular}
\end{table}

In total, $400\,000$ training observations are simulated from random pairs of COSMOS sources 
and IllustrisTNG convergence maps in order to train the RIM. 
An additional $200\,000$ observations are created from pairs 
of COSMOS sources and pixelated SIE convergence maps. 
The parameters for these $\kappa$ maps are listed in table \ref{tab:sie}. 
% We expect some lensing configurations like doubly imaged systems to poorly constrain the inner structure of the 
% mass distribution. 
% Building an inference pipeline with strong constraints (other than the lensed image) on the 
% slope of the profile goes beyond the scope of this work. 
% As such, imposing an implicit prior for the slope through 
% the dataset is sufficient for our goal. 
% It is also motivated by the \textit{bulge-halo conspiracy} --- 
% the observation that most lensing configurations observed in the sky can be explained 
% to a first order approximation by 
% an average slope consistent with an isothermal profile \citep{Auger2010,Dutton2014}.

\begin{table}[htb!]
\centering
\caption{SIE parameters.}
\label{tab:sie}
\begin{tabular}{ccc}
        Parameter &  Distribution \\
        \hline \hline
         Radial shift ('') & $\mathcal{U}(0, 0.1)$ \\
        Azimutal shift & $\mathcal{U}(0, 2\pi)$ \\
        Orientation & $\mathcal{U}(0, \pi)$ \\
        $\theta_E$ ('') & $\mathcal{U}(0.5, 2.5)$ \\
        Ellipticity & $\mathcal{U}(0, 0.6)$ \\
        \hline
\end{tabular}
\end{table}


We generate $1\,600\,000$ simulated observations from the VAE 
background sources and convergence maps as part of the training set. We apply some validation checks to each example in order to avoid configurations like a single image of the background source or an Einstein ring cropped by the field of view.
% In principle, we could generate as many simulated observations as needed from the VAE. 
% However, having a set of finite, fixed size lets us apply some validation checks to each 
% example in order to avoid configurations like a
% single image of the background 
% source or an Einstein ring cropped by the field of view.




\section{Training}\label{sec:training}

\subsection{VAE}\label{sec:vae training}
Here, we describe the training of two VAEs that are used to produce density maps and images of unlensed background galaxies to train and test our inference model. For an introduction to VAEs we refer the reader to \citet{Kingma2019}.

As mentioned in \citet{Kingma2019}, direct optimisation 
of the ELBO loss can prove difficult because the reconstruction term $\log p_\theta (\mathbf{x} \mid \mathbf{z})$ 
is relatively weak compared to the Kullback Leibler (KL) divergence term. To alleviate this issue, 
we follow the work of \citet{Bowman2015} and \citet{Sonderby2016} in setting a warm-up 
schedule for the KL term, starting from $\beta=0.1$ up to $\beta_{\mathrm{max}}$. 

Usually, 
$\beta_{\mathrm{max}} = 1$ is considered optimal since it matches the original ELBO  
objective derived by \citet{Kingma2013}. 
However, we are more interested in the 
sharpness of our samples and accurate inference around small regions of the latent 
space for fine-tuning. Thus, setting $\beta_{\mathrm{max}} < 1$ allows us to increase 
the size of the information bottleneck (i.e.~latent space) of the VAE 
and improve the reconstruction cost of the model. 
This is a variant of the $\beta$-VAE \citep{Higgins2017}, where $\beta > 1$ was found 
to improve disentangling of the latent space \citep{Burgess2018}. 

The value for $\beta_\mathrm{max}$ and the steepness of the schedule 
are grid searched alongside the architecture for the VAE. 
These values are found in practice by 
manually looking at the quality of generated samples for different VAE 
hyperparameters. A similar method is explored and formalized in the 
InfoVAE framework \citep{Zhao2017}.


A notable element of the VAE architecture is the use of a fully connected
layer to reshape the features of the convolutional layer into the chosen 
latent space dimension. Following the work of \citet{Lanusse2021}, we introduce 
an $\ell_{2}$ penalty between the input and output of the bottleneck 
dense layers to encourage an identity mapping. This regularisation 
term is slowly removed during training.


\subsection{RIM}\label{sec:rimtraining}

The architecture of the neural network was grid searched on 
a smaller dataset ($\lesssim 10\,000$ examples) 
in order to quickly identify a small set
of valid hyperparameters. Then, the best hyperparameters were 
identified using a two-stage training process on the training dataset. 
In the first stage, we trained 24 different architectures from this small hyperparameter set for approximately 4 days (wall time using a single Nvidia A100 GPU). 
Different architectures would have a training time much longer than others, and this 
was factored in the architecture selection process. For example, adding more time steps ($T$) to the recurrent relation \eqref{eq:RIM} 
would yield better generalisation on the test set, but this 
would come at great costs to training time until convergence. 

Following this first stage, 4 architectures were deemed efficient enough 
to be trained for an additional 6 days. 
We only report the results for the best architectures out of these 4.
%We report the hyperparameters for 
%the best architecture after this second stage of training in table \ref{tab:baseline hparams}.

Each reconstruction is performed by fine-tuning the baseline model 
on a test task composed of an observation vector, a PSF, and a noise covariance.
In practice, fine-tuning predictions on the test set of $3\,000$ examples can be accomplished in parallel so as to be done in at most a few days by spreading the computation on $\sim 10$ Nvidia A100 GPUs. Each reconstruction uses at most 2000 steps, correspondling to approximately $20$ minutes (wall-time) per reconstruction. Early stopping is applied when the $\chi^2$ reaches noise level. The hyperparameters for this procedure are reported in Table \ref{tab:fine-tuning hparams}.


\begin{table}[H]
        \centering
        \caption{Hyperparameters for fine-tuning the RIM.}
        \label{tab:fine-tuning hparams}
        \begin{tabular}{cc}
                Parameter & Value \\\hline\hline
                Optimizer & RMSProp \\
                Learning rate & $10^{-6}$\\
                Maximum number of steps & $2\,000$\\
                $\lambda$ & $2\times 10^{5}$\\
                $\ell_2$ & 0 \\
                Number of samples from VAE & 200 \\
                Latent space distribution & $\mathcal{N}(\mathbf{z}^{(T)}, \sigma=0.3)$
                \footnote{$\mathbf{z}^{(T)}$ is the latent code of the RIM baseline source or convergence.}\\
                \hline
        \end{tabular}
\end{table}



\begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{figures/main_result}
        \caption{
                Sample of the fine-tuned RIM reconstructions 
                on a test set of 3000 examples. 
                Examples are ordered from the best $\chi^2$ (top) to the worst (bottom). 
                The percentile rank of each example is in the leftmost column. 
                The last example 
        shown has SNR above the threshold defined in Figure \ref{fig:chi squared vs noise}.}
        \label{fig:main result}
        %\vspace{-1.5pt} % fix
\end{figure}

\begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{figures/test_set_no_cherry_pick}
        \caption{
                30 reconstructions taken at random from the test set of 3000 examples simulated from COSMOS 
                and IllustrisTNG data at high SNR.
                The colorscale are the same as in Figure \ref{fig:main result}.}
        \label{fig:random sample}
	\vspace{-1.5pt} % fix
\end{figure}


\section{Results}\label{sec:results}

In this section, we present the performance of our model 
on the held out test set. A sample of 3000 reconstruction 
problems is generated from the held-out \textit{HST} and IllustrisTNG data 
with noise levels and PSFs similar to the training set.

\subsection{Goodness of Fit}
Figure \ref{fig:main result} shows a sample of reconstructions for high SNR data with a wide range of lensing configurations from the test set.
We select examples representative of all levels of reconstruction performance (covering the entire range of goodness of fit) for data with  complex structures in their convergence map to showcase the expressivity of the approach. 
We also show a randomly selected sample from the test set in Figure \ref{fig:random sample}.

\begin{table}[H]
    \centering
    \caption{$\log_{10}$-normal moments of the loss on the test set}
    \label{tab:loss}
    \begin{tabular}{ccc}
        \hline
          Model  & $\mu(\log \mathcal{L}_\varphi)$ & $\sigma(\log \mathcal{L}_\varphi)$ \\
        \hline \hline
        Baseline ($\varphi_{\mathcal{D}}^\star)$ &  -1.96 & 0.36 \\
        Fine-tuned ($\hat{\varphi}_{\mathrm{MAP}}$) & -2.02 & 0.37 \\\hline
    \end{tabular}
\end{table}


\begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/loss_and_likelihood}
        \caption{Distribution of the goodness of fit for the baseline and fine-tuned network (right panel), as well as log-loss difference between the two network for a given example in the test set (left panel).
}
        \label{fig:loss and chi squared}
\end{figure}


Figure \ref{fig:loss and chi squared} shows a comparison between 
the goodness of fit of the baseline model and the fine-tuned prediction. 
Since we empirically observe that the distribution of the loss on the test set (and the training set) follows a log-normal distribution, we find that it is more informative to look at the $\log$-loss 
distribution to extract information about the fine-tuning procedure. 
The left panel of Figure \ref{fig:loss and chi squared} 
shows the distribution of the log-loss difference between the fine-tuned prediction and the baseline model. This distribution shows that the fine-tuning procedure loss is constrained within $\sim 1$ order of magnitude of the original loss with a probability $>99.73\,\%$. We find that the log-loss difference has a scatter of $\sigma = 0.28$, which is smaller than the scatter of the baseline log-loss over the entire test set $\sigma(\log \mathcal{L}_{\varphi^\star_{\mathcal{D}}}) = 0.36$ reported in Table \ref{tab:loss}.
We note that the loss is not optimized during fine-tuning, still we notice that the fine-tuning procedure does not significantly deteriorate or improve the loss of the baseline prediction on average. We report the first 2 moments of the loss log-normal distribution for the baseline and the fine-tuned reconstructions in Table \ref{tab:loss} in order to explicitly compare them. As can be seen in this table, there is no significant difference between the two distributions. This statement can be proven for the measured mean values --- $\mu(\log \mathcal{L}_{\hat{\varphi}_{\mathrm{MAP}}}) = \mu(\log \mathcal{L}_{\varphi^{\star}_{\mathcal{D}}}) $ --- using the two-sided normal p-value test \citep{Casella2001}, which we find satisfy the null hypothesis with $p=0.87288$ ($Z = -0.16$). All those observations support our claim that EWC regularisation preserves the prior learned during pretraining, or at least that it preserves the surrogate measures of the prior we reported. 

\begin{figure}[H]
        \centering
        \includegraphics[width=0.6\linewidth]{figures/chisq_vs_noise_ewc}
        \caption{Goodness of fit as a function of SNR shows a threshold 
        behavior where our method reaches its limit.}
        \label{fig:chi squared vs noise}
\end{figure}



The right panel of Figure \ref{fig:loss and chi squared} shows the distribution of  $\chi^2$ for the test set before and after the fine-tuning procedure and the theoretical $\chi^2$ distribution corresponding to $\nu=128^2$ degrees of freedom.
We observe that the fine-tuning procedure significantly improves our $\chi2$, bringing their distribution closer to that of the expected $\chi^2$ distribution (black curve). However, the improved distribution is still far from the theoretical expectation, implying that there are statistically significant residuals in a subset of the reconstructions.

In figure \ref{fig:chi squared vs noise}, we explore how the goodness of fit of the fine-tuned RIM changes as a function of SNR over the examples in the test set. Two behaviors can be identified. For SNR below a certain threshold, the goodness of fit 
of the fine-tuned model is essentially flat, with a certain scatter, around the noise level. This scatter increases as a function of SNR, which reflects the fact that above a certain SNR threshold (vertical dashed line in Figure \ref{fig:chi squared vs noise}), our reconstructions are dominated by systematics in the inference algorithm.
For SNR above the threshold, 
the goodness of fit follows the trend $\chi^2 \propto \sigma^{-2}$ (the solid line in Figure \ref{fig:chi squared vs noise}), which 
means the reconstructions have stopped improving on par with the SNR.

This behavior is exhibited in a few examples of reconstructions taken from the test set in Figure \ref{fig:increasing SNR}, where we ordered reconstructions with increasing SNR from top to bottom and plotted the surface brightness and foreground densities in log scale. As can been seen, errors in reconstructed parameters remain of the same order of magnitude as SNR is increased from $\sim$220 to 500, implying that above this SNR threshold, the reconstructions are dominated by systematics. 

\begin{figure}[H]
        \centering
        \tikzset{font={\fontsize{8pt}{12}\selectfont}}
        \begin{tikzpicture}
                \node at (0, 0) {\includegraphics[width=\linewidth]{figures/rim_map_wrt_snr.pdf}};
                \draw[-latex] (-8.5, 3.5) -- (-8.5, -3.5) node[midway, above, rotate=90] {Increasing SNR};
                \node at (-5.5, 4.5) {\strut Source};
                \node at (-0.75, 4.5) {\strut Convergence};
                \node at (5, 4.5) {\strut Residuals};
        \end{tikzpicture}
        \caption{
        Comparison between baseline (RIM) and fine-tuned (RIM+FT) reconstructions for gravitational lensing systems from the test set (GT).
        From top to bottom, we increase SNR. 
        }
        \label{fig:increasing SNR}
\end{figure}
%\vfill\null 

\subsection{Quality of the Reconstructions}\label{sec:quality of reconstructions}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{figures/coherence_spectrum}
        \caption{Statistics of the coherence spectrum on the test set. The solid line is the average 
        coherence. The transparent region is the $68\%$ confidence interval. The fine-tuning 
        procedure yields a noticeable improvement on the coherence of the source at all frequencies.}
        \label{fig:coherence}
\end{figure}


In addition to a visual inspection of the reconstructed sources 
and convergences, we compute 
the coherence spectrum to quantitatively assess the quality of the reconstructions
\begin{equation}\label{eq:coherence} 
        \gamma(k) = \frac{P_{12}(k)}{\sqrt{P_{11}(k) P_{22}(k)}} \, .
\end{equation}

Here, $P_{ij}(k)$ is the cross power spectrum of images $i$ and $j$ at 
the wavenumber $k$. Figure \ref{fig:coherence} shows the mean value and the $68\%$ inclusion interval of $\gamma(k)$ 
for the convergence and source maps in a test set of 3000 examples. 
The fine-tuning 
procedure, shown in red, is able to significantly improve the coherence of the baseline background 
source, shown in black, at all scales. 
The coherence spectrum of the convergence sees a slight improvement due to the fine-tuning procedure.
Still, we note that many examples in the dataset exhibit significant 
improvement, which we illustrate in Figure \ref{fig:main figure}.
%In this case, the observed lensed image exhibits a large deflection in 
%its eastern arc which indicates the presence of a massive object --- in this 
%case a dark matter subhalo. The fine-tuning procedure is able to recover 
%this subhalo because of its strong signal in the lensed image.


\section{Conclusion}\label{sec:conclusion}
The results obtained here demonstrate the effectiveness of machine learning methods, specifically a recurrent inference machine, for inferring pixelated maps of the distribution of mass in lensing galaxies and the distribution of surface brightness in the background galaxies. Since this is a heavily under-constrained problem, stringent priors are needed to avoid overfitting the data, a task that has traditionally been difficult to accomplish with traditional statistical models \citep[e.g., ][]{Saha1997}. The model proposed here can implicitly learn these priors from a set of training data. 

The fine-tuning step that we propose in this work is a general procedure (i.e.\ not specific to our model or problem), which enables us to exploit a diagonal second-order Laplace approximation of the implicit prior learned by a baseline estimator during pre-training. We use fine-tuning in order to significantly improve this baseline estimator (i.e., a better MAP estimate), by using the likelihood of the data and the EWC prior. In the context of our work, we find that fine-tuning has a limiting --- or threshold --- behavior, which we speculate is due to the limited expressivity of the neural network and its inductive biases learned during pre-training.

The flexible and expressive form of the reconstructions shown in this work means that, in principle, any lensing system (e.g., a single simple galaxy or a group of complex galaxies) could be analyzed by this model, without any need for pre-determining the model parameterization. This is of high value given the diversity of observed lensing systems, and their relevance for constraining astrophysical and cosmological parameters. 

Perhaps the most important limitation of the method is the fact that, in its current form, the model only provides point estimates of the parameters of interest. Quantifying the posteriors of such high-dimensional data will require an efficient and accurate generative process \citep[e.g., see ][]{Adam:22a}, which we plan to explore and develop in future works.

\section*{Software and data}
The source code, as well as the various scripts and parameters used to 
produce the model and results is available as open-source software 
under the package \texttt{Censai}\footnote{
\href{https://github.com/AlexandreAdam/Censai}{
\includegraphics[scale=0.25]{figures/GitHub-Mark-32px.png}
https://github.com/AlexandreAdam/Censai}}. 
The model parameters, as well as convergence maps used to train 
these models and the test set examples and reconstructions results are also available as open-source datasets hosted by Zenodo\footnote{\href{https://doi.org/10.5281/zenodo.6555463}
{\includegraphics[scale=0.1]{figures/zenodo}
https://doi.org/10.5281/zenodo.6555463}}. This research made use of \texttt{Tensorflow} \citep{tensorflow}, 
\texttt{Tensorflow-Probability} \citep{tensorflow-probability}, 
\texttt{Numpy} \citep{numpy}, 
\texttt{Scipy} \citep{scipy}, 
\texttt{Matplotlib} \citep{matplotlib}, 
\texttt{Scikit-image} \citep{scikit-image}, 
\texttt{IPython} \citep{ipython}, 
\texttt{Pandas} \citep{pandas1,pandas2}, 
\texttt{Scikit-learn} \citep{scikit-learn}, 
\texttt{Astropy} \citep{astropy:2013,astropy:2018} 
and \texttt{GalSim} \citep{galsim}.

\section*{Acknowledgements}

This research was made possible by a generous donation by Eric and Wendy Schmidt with the recommendation of the Schmidt Futures Foundation.

We would like to thank Ronan Legin for fruitful discussions and insights about training the neural network. We would also like to thank Max Welling for insightful comments on our work.  The work is in part supported by computational resources provided by Calcul Quebec, Compute Canada and the Digital Research Alliance of Canada. Y.H. and L.P. acknowledge support from the National Sciences and Engineering Council of Canada grant RGPIN-2020-05102, the Fonds de recherche du Québec grant 2022-NC-301305 and 300397, and the Canada Research Chairs Program. A.A. was supported by an IVADO Excellence Scholarship.


